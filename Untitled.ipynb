{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. changed runtime from 5 to 10\n",
    "2. increased the depth of the network from 2 to 3 and hidden layer from 64 to 512 to 128\n",
    "3. increased the learning rate of NN from 0.0001 to 0.001\n",
    "4. changed decay rate from 0.0001 to 0.001\n",
    "5. memory batch size changed from 20 to 30 to 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEScgl5EJOL+P13RjLI+Dni6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64acbfa320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 100/np.exp(np.linspace(10,0,10)) # np.arange(1000)\n",
    "\n",
    "plt.plot(a)\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10.,  109.,  208.,  307.,  406.,  504.,  603.,  702.,  801.,  900.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.linspace(10,900,10),0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "# for i in itertools.product([1,2,3], repeat=4):\n",
    "#     print(i)\n",
    "    \n",
    "{index:val for index, val in enumerate(itertools.product([1,2,3], repeat=4))}\n",
    "\n",
    "np.random.choice(np.arange(3),size=4)\n",
    "\n",
    "np.arange(12).reshape(3,4).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([1,2,3])\n",
    "a= [1,2,3]\n",
    "b=[2,2,3]\n",
    "a==b\n",
    "# type(a==b)\n",
    "# np.max(np.arange(12).reshape(3,4),axis=1)\n",
    "# np.arange(12).reshape(3,4)\n",
    "\n",
    "(np.array([1.,2.,3.,4.])==np.round(np.array([1.23,2.34,3.27,4.5]),0)).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 2]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.permutations([1, 2, 3]))\n",
    "\n",
    "\n",
    "# def perm(n, seq):\n",
    "#     for p in itertools.product(seq, repeat=n):\n",
    "#         print(p)\n",
    "\n",
    "# len(perm(3, \"012\"))\n",
    "a = np.arange(3)\n",
    "\n",
    "# print(len(list(itertools.product(a, repeat=4))))\n",
    "# print(list(itertools.product(a, repeat=4)))\n",
    "\n",
    "dict_index_to_levels = {index: val for index, val in enumerate(itertools.product(a, repeat=4))}\n",
    "dict_levels_to_index = {val:index for index,val in w_index_to_levels.items()}\n",
    "list(dict_index_to_levels[2])\n",
    "# {val:index for index, val in enumerate(itertools.product(a, repeat=4))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [-1,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "np.sum(a)\n",
    "rotor_speeds = np.exp(np.linspace(0,np.log(900),20))\n",
    "# rotor_speeds[0] = 0\n",
    "rotor_speeds = np.round(np.linspace(100,900,9),0)\n",
    "rotor_speeds\n",
    "# np.exp(a)\n",
    "# np.square(a)\n",
    "\n",
    "# np.random.choice(np.arange(21),size=4)\n",
    "# a= np.array([-1,32,21.1])\n",
    "# b = np.array([-1,32.,21])\n",
    "# (a==b).all()\n",
    "# np.round(np.linspace(400,500,3),0)  \n",
    "[np.random.choice(9)+1]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1052.27393126,  868.58304085,  905.42730422, 1079.41813688],\n",
       "       [1094.9188841 , 1116.01229469,  960.72167414,  909.26390279],\n",
       "       [ 816.53371163, 1019.35824034, 1033.03494263,  775.60920809]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(3,4),scale=100,loc=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 1, 2, 3, 0, 1, 2, 3, 1, 2, 3, 0, 1, 2, 3, 1, 2, 3, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([1,2,3])\n",
    "    \n",
    "np.concatenate([np.concatenate([a,b,[False]])] * 3) \n",
    "# np.append(a,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]), array([1, 2, 3])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([1,2,3] +[3])*2\n",
    "a = np.array([1,2,3])\n",
    "[a]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "z= 0\n",
    "\n",
    "max_height_reward = 5       #50 is max reward after takeoff upon reaching max desired height\n",
    "max_height = 50\n",
    "max_height_acheived = True\n",
    "\n",
    "factor = np.pi/max_height\n",
    "res = z*factor\n",
    "\n",
    "if z <= max_height and not max_height_acheived:\n",
    "    reward = max_height_reward*round(np.sin(res/2.0),4)\n",
    "    print(reward)         #divide by 10 to reduce rewards scale\n",
    "else:\n",
    "    max_height_acheived = True\n",
    "    reward = max_height_reward*round(np.cos(res/2.0),4)\n",
    "    print(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 3, 4, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[1,2,3],[3,4,5]],axis = 0)\n",
    "\n",
    "# np.concatenate([np.arange(12).reshape(3,4),np.arange(12).reshape(3,4)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloHello\n"
     ]
    }
   ],
   "source": [
    "def call(str):\n",
    "    for _ in range(3):\n",
    "        done = str*_\n",
    "    print(done)\n",
    "call('Hello')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "task.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "           \n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def calculate_reward(self, pose):\n",
    "\n",
    "        z = pose[2]\n",
    "\n",
    "        if z <= self.max_height and not self.max_height_acheived:\n",
    "            if self.sim.v[2] < 0:\n",
    "                reward = -5*(abs(self.sim.pose[2] - self.max_height))\n",
    "            else:\n",
    "                reward = -.1*(abs(self.sim.pose[2] - self.max_height))   \n",
    "            \n",
    "        else:\n",
    "            if self.sim.v[2] > 0:\n",
    "                reward = -10000 # -5*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "            else:\n",
    "                reward = -.1*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "            \n",
    "            if not self.max_height_acheived:\n",
    "                reward = reward + 10000\n",
    "            \n",
    "            self.max_height_acheived = True\n",
    "            #reward = reward + (1.-.5*(abs(self.sim.v - self.target_velocity)).sum())\n",
    "        \n",
    "            #if (self.sim.pose==self.target_pos):# and (self.sim.v==self.target_velocity):\n",
    "            if (np.round(np.array(self.sim.pose),0)==np.array(self.target_pos)):\n",
    "                reward = reward + 10000\n",
    "                self.Has_landed = True\n",
    "                print('Egle Has landed...')\n",
    "    \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from physics_sim import PhysicsSim\n",
    "import itertools\n",
    "\n",
    "class Task():\n",
    "    \"\"\"Task (environment) that defines the goal and provides feedback to the agent.\"\"\"\n",
    "    def __init__(self, init_pose=None, init_velocities=None, \n",
    "        init_angle_velocities=None, runtime=5., target_pos=None):\n",
    "        \"\"\"Initialize a Task object.\n",
    "        Params\n",
    "        ======\n",
    "            init_pose: initial position of the quadcopter in (x,y,z) dimensions and the Euler angles\n",
    "            init_velocities: initial velocity of the quadcopter in (x,y,z) dimensions\n",
    "            init_angle_velocities: initial radians/second for each of the three Euler angles\n",
    "            runtime: time limit for each episode\n",
    "            target_pos: target/goal (x,y,z) position for the agent\n",
    "        \"\"\"\n",
    "        # Simulation\n",
    "        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime) \n",
    "        self.action_repeat = 3\n",
    "\n",
    "        self.state_size = self.action_repeat * 9\n",
    "        self.action_low = 100\n",
    "        self.action_high = 900\n",
    "        self.action_size = 4\n",
    "        self.action_split = 9 # 3 SPEED LEVELS 400,450,500\n",
    "\n",
    "        # Goal\n",
    "        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.]) \n",
    "        \n",
    "        self.max_height_reward = 5       #5 is max reward after takeoff upon reaching max desired height\n",
    "        self.max_height = 20\n",
    "        self.max_height_acheived = False\n",
    "        self.max_height_target = np.array([0., 0., 30.])\n",
    "        self.target_velocity =  np.array([0., 0., 0.])\n",
    "        self.Has_landed = False\n",
    "        \n",
    "        a = np.arange(self.action_split)\n",
    "        self.dict_index_to_levels = {index:val for index, val in enumerate(itertools.product(a, repeat=4))}\n",
    "        self.dict_levels_to_index = {val:index for index,val in self.dict_index_to_levels.items()}\n",
    "        \n",
    "        self.rotor_speed_levels =  np.round(np.linspace(self.action_low,self.action_high,self.action_split),0)\n",
    "\n",
    "    def get_reward(self,done):\n",
    "        \"\"\"Uses current pose of sim to return reward.\"\"\"\n",
    "#         reward = 1.-.3*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "        if done:\n",
    "            if self.sim.time < 10.:\n",
    "                reward = -100000\n",
    "            else:\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = self.calculate_reward(self.sim.pose)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def step(self, rotor_speeds):\n",
    "        \"\"\"Uses action to obtain next state, reward, done.\"\"\"\n",
    "        reward = 0\n",
    "        pose_all = []\n",
    "        \n",
    "        rotor_speeds = self.eval_rotor_speed(rotor_speeds)\n",
    "        \n",
    "        for _ in range(self.action_repeat):\n",
    "            done = self.sim.next_timestep(rotor_speeds) # update the sim pose and velocities\n",
    "            reward += self.get_reward(done) \n",
    "            pose_all.append(np.concatenate([self.sim.pose,self.sim.v,[self.max_height_acheived]]))\n",
    "            \n",
    "        if done or self.Has_landed:\n",
    "            next_state = self.reset()\n",
    "            done = True\n",
    "        else:\n",
    "            next_state = np.concatenate(pose_all)\n",
    "            \n",
    "#         print('next state',next_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def eval_rotor_speed(self,rotor_speeds):\n",
    "\n",
    "        calc_rotor_speed = [self.rotor_speed_levels[idx] for idx in rotor_speeds]\n",
    "        #print(calc_rotor_speed)\n",
    "        return calc_rotor_speed\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the sim to start a new episode.\"\"\"\n",
    "        self.sim.reset()\n",
    "        state = np.concatenate([np.concatenate([self.sim.pose,self.sim.v,[self.max_height_acheived]])] * self.action_repeat)\n",
    "        self.max_height_acheived = False\n",
    "        self.Has_landed = False\n",
    "        return state\n",
    "    \n",
    "    def calculate_reward(self, pose):\n",
    "\n",
    "        z = pose[2]\n",
    "\n",
    "        if z <= self.max_height and not self.max_height_acheived:\n",
    "            \n",
    "            #if self.sim.v[2] < 0:\n",
    "             #   reward = -5*(abs(self.sim.pose[2] - self.max_height))\n",
    "            #else:\n",
    "            reward = 10/np.exp(abs(self.sim.pose[2] - self.max_height))   \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #if self.sim.v[2] > 0:\n",
    "             #   reward = -10000 # -5*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "            #else:\n",
    "            reward = 100/np.exp((abs(self.sim.pose[:3] - self.target_pos)).sum())\n",
    "            \n",
    "            #if not self.max_height_acheived:\n",
    "            #    reward = reward + 10000\n",
    "            \n",
    "            self.max_height_acheived = True\n",
    "            #reward = reward + (1.-.5*(abs(self.sim.v - self.target_velocity)).sum())\n",
    "        \n",
    "            #if (self.sim.pose==self.target_pos):# and (self.sim.v==self.target_velocity):\n",
    "            if (np.round(np.array(self.sim.pose),0)==np.array(self.target_pos)):\n",
    "                reward = reward+ 1000\n",
    "                self.Has_landed = True\n",
    "                print('Egle Has landed...')\n",
    "            else:\n",
    "                reward = 0\n",
    "    \n",
    "        return reward\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    " #         rotor_speeds = np.exp(np.linspace(0,np.log(900),20)) #rotor speed broken down into 20 levels changing exponentialy...\n",
    "    \n",
    "#     def calculate_reward(self, pose):\n",
    "\n",
    "#         z = pose[2]\n",
    "# #         factor = np.pi/max_height\n",
    "# #         res = z*factor\n",
    "\n",
    "#         if z <= max_height and not max_height_acheived:\n",
    "# #             reward = max_height_reward*round(np.sin(res/2.0),4)\n",
    "#             reward = 1.-.5*(abs(self.sim.pose[:3] - self.max_height_target)).sum()    \n",
    "#             print(reward)         \n",
    "#         else:\n",
    "#             max_height_acheived = True\n",
    "# #             reward = max_height_reward*round(np.cos(res/2.0),4)\n",
    "#             reward = 1.-.5*(abs(self.sim.pose[:3] - self.target_pos)).sum()\n",
    "    \n",
    "#             reward = reward + (1.-.5*(abs(self.sim.v - self.target_velocity)).sum())\n",
    "        \n",
    "#             if np.sum(self.sim.pose) == 0 and np.sum(self.sim.v) == 0:\n",
    "#                 reward = reward + 100\n",
    "#                 self.Has_landed = True\n",
    "    \n",
    "# #             if z < 5 and np.sum(self.sim.v) > 10: #penalize the action if at lower height velocities are high.\n",
    "# #                 reward = reward - 10\n",
    "                \n",
    "#             print(reward)\n",
    "            \n",
    "#         return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "# import pandas as pd\n",
    "# from agents.policy_search import PolicySearch_Agent\n",
    "# from task import Task\n",
    "\n",
    "# num_episodes = 1000\n",
    "# target_pos = np.array([0., 0., 10.])\n",
    "# task = Task(target_pos=target_pos)\n",
    "# agent = PolicySearch_Agent(task) \n",
    "\n",
    "# for i_episode in range(1, num_episodes+1):\n",
    "#     state = agent.reset_episode() # start a new episode\n",
    "#     while True:\n",
    "#         action = agent.act(state) \n",
    "#         next_state, reward, done = task.step(action)\n",
    "#         agent.step(reward, done)\n",
    "#         state = next_state\n",
    "#         if done:\n",
    "#             print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "#                 i_episode, agent.score, agent.best_score, agent.noise_scale))  # [debug]\n",
    "#             break\n",
    "#     sys.stdout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
